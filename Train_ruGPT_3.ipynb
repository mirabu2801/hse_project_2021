{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_ruGPT_3 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "asqMueYPeIgK"
      },
      "source": [
        "!pip3 install urllib3==1.25.4\n",
        "!pip install alignment\n",
        "!pip3 install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA6mAzlexsHp"
      },
      "source": [
        "def train_test_files_split(f='df_only_latest.txt'):\n",
        "  file = open(f)\n",
        "  lines = file.read().split('\\n')\n",
        "  test_size = int(len(lines) * 0.15)\n",
        "  test = lines[:test_size]\n",
        "  train = lines[test_size:]\n",
        "  train_file = open('train.txt', 'w')\n",
        "  test_file = open('test.txt', 'w')\n",
        "  print('\\n'.join(test), file=test_file)\n",
        "  print('\\n'.join(train), file=train_file)\n",
        "  test_file.close()\n",
        "  train_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2wrrPtw1i3Q"
      },
      "source": [
        "train_test_files_split('df.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vL07XFvsBBU"
      },
      "source": [
        "#sberbank-ai/rugpt3medium_based_on_gpt2\n",
        "!mkdir gpt\n",
        "!python pretrain_transformers.py \\\n",
        "    --output_dir=/content/drive/MyDrive/gpt \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=sberbank-ai/rugpt3large_based_on_gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=test.txt \\\n",
        "    --per_gpu_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 5 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_train_epochs 40 \\\n",
        "    --block_size 120 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --line_by_line \\\n",
        "    --save_steps 1000000 \\\n",
        "    --check_acc_step 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnTGExQR4YUX",
        "outputId": "44e8869f-debb-4ecf-db6f-12ea7caeedf4"
      },
      "source": [
        "!mkdir gpt2\n",
        "!python pretrain_transformers.py \\\n",
        "    --output_dir=gpt2 \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt \\\n",
        "    --do_train \\\n",
        "    --train_data_file=train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=test.txt \\\n",
        "    --per_gpu_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 5 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_train_epochs 40 \\\n",
        "    --block_size 120 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --line_by_line \\\n",
        "    --save_steps 1000000 \\\n",
        "    --check_acc_step 1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQNsedMyL4DZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AX_B6yVia54"
      },
      "source": [
        "model = model.cuda() # Выполнить, если доступно gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNOjKDF3A-aO"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ7-af0teplO"
      },
      "source": [
        "tokens = []\n",
        "simple = ['я', 'мы', 'нас', 'нами', 'мне', 'мой', 'мое', 'моё', 'моего', 'наш', 'нашего', 'нашему', 'мной', 'нами']\n",
        "for word in simple:\n",
        "  options = []\n",
        "  options.append(tokenizer.encode(word)[0])\n",
        "  options.append(tokenizer.encode(word.capitalize())[0])\n",
        "  options.append(tokenizer.encode(' ' + word)[0])\n",
        "  options.append(tokenizer.encode(' ' + word.capitalize())[0])\n",
        "  tokens.extend(options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7IVYtAcepui"
      },
      "source": [
        "tokens = []\n",
        "difficult = ['он', 'они', 'их', 'ими', 'ему', 'его', 'ей', 'ее', 'её', 'им', 'него', 'нее', 'неё']\n",
        "for word in difficult:\n",
        "  options = []\n",
        "  options.append(tokenizer.encode(word)[0])\n",
        "  options.append(tokenizer.encode(word.capitalize())[0])\n",
        "  options.append(tokenizer.encode(' ' + word)[0])\n",
        "  options.append(tokenizer.encode(' ' + word.capitalize())[0])\n",
        "  tokens.extend(options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ksQ1eUmBmk",
        "outputId": "6569ca7e-9078-4799-e3f7-4d611029912d"
      },
      "source": [
        "from alignment.sequence import Sequence\n",
        "from alignment.vocabulary import Vocabulary\n",
        "from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner\n",
        "\n",
        "def align(a, b):\n",
        "    a =  Sequence([-1, -1, -1, -1] + a + [-1, -1, -1, -1])\n",
        "    b = Sequence([-1, -1, -1, -1] + b + [-1, -1, -1, -1])\n",
        "    v = Vocabulary()\n",
        "    aEncoded = v.encodeSequence(a)\n",
        "    bEncoded = v.encodeSequence(b)\n",
        "    scoring = SimpleScoring(2, -1)\n",
        "    aligner = GlobalSequenceAligner(scoring, -2)\n",
        "    score, encodeds = aligner.align(aEncoded, bEncoded, backtrace=True)\n",
        "    p = v.decodeSequenceAlignment(encodeds[0])\n",
        "    return p.first[4:-4], p.second[4:-4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5, 0.5, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoZ3BZrjuO6w"
      },
      "source": [
        "MAX_LENGTH = int(10000)\n",
        "def adjust_length_to_model(length, max_sequence_length):\n",
        "    if length < 0 and max_sequence_length > 0:\n",
        "        length = max_sequence_length\n",
        "    elif 0 < max_sequence_length < length:\n",
        "        length = max_sequence_length  # No generation bigger than model size\n",
        "    elif length < 0:\n",
        "        length = MAX_LENGTH  # avoid infinite loop\n",
        "    return length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pej_eCIbuOxl"
      },
      "source": [
        "repetition_penalty = 1.0\n",
        "device = 'cuda' # Поменять при необходимости\n",
        "k = 50\n",
        "p = 0.5\n",
        "num_return_sequences = 1\n",
        "stop_token = '</s>'\n",
        "\n",
        "def predict_raw(model, tokenizer, prompt_text):\n",
        "  length = adjust_length_to_model(100, max_sequence_length=model.config.max_position_embeddings)\n",
        "  temperature = 1.0\n",
        "  generated_sequences = []\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(device)\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=length + len(encoded_prompt[0]),\n",
        "      temperature=temperature,\n",
        "      top_k=k,\n",
        "      top_p=p,\n",
        "      repetition_penalty=repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "  )\n",
        "  if len(output_sequences.shape) > 2:\n",
        "      output_sequences.squeeze_()\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "      generated_sequence = generated_sequence.tolist()\n",
        "      text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "      text = text[: text.find(stop_token) if stop_token else None]\n",
        "      total_sequence = (\n",
        "          prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "      )\n",
        "      generated_sequences.append(total_sequence)\n",
        "  return generated_sequences[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR1AAe30zo11"
      },
      "source": [
        "def predict(text, model=model, tokenizer=tokenizer):\n",
        "  return predict_raw(model, tokenizer, f'in: {text} [end] out: ').split('[end] out: ')[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IYOzVTr_Xc-"
      },
      "source": [
        "import torch\n",
        "from alignment.sequence import Sequence\n",
        "from alignment.vocabulary import Vocabulary\n",
        "from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner\n",
        "\n",
        "def align(a, b):\n",
        "  a =  Sequence([-1, -1, -1, -1] + a + [-1, -1, -1, -1])\n",
        "  b = Sequence([-1, -1, -1, -1] + b + [-1, -1, -1, -1])\n",
        "  v = Vocabulary()\n",
        "  aEncoded = v.encodeSequence(a)\n",
        "  bEncoded = v.encodeSequence(b)\n",
        "\n",
        "  scoring = SimpleScoring(2, -1)\n",
        "  aligner = GlobalSequenceAligner(scoring, -2)\n",
        "  score, encodeds = aligner.align(aEncoded, bEncoded, backtrace=True)\n",
        "\n",
        "  p = v.decodeSequenceAlignment(encodeds[0])\n",
        "  return p.first[4:-4], p.second[4:-4]\n",
        "\n",
        "def split_by_words(sentence):\n",
        "  words = []\n",
        "  i = 0\n",
        "  last_stop = 0\n",
        "\n",
        "  while i < len(sentence):\n",
        "    if sentence[i].isalpha():\n",
        "      i += 1\n",
        "      continue\n",
        "    elif sentence[i].isspace():\n",
        "      if last_stop < i:\n",
        "        words.append(sentence[last_stop:i])\n",
        "      i += 1\n",
        "      last_stop = i\n",
        "    else:\n",
        "      if last_stop < i:\n",
        "        words.append(sentence[last_stop:i])\n",
        "      words.append(sentence[i])\n",
        "      i += 1\n",
        "      last_stop = i\n",
        "  return words\n",
        "\n",
        "def get_pair(a, b, pad_id = 0, device='cuda'):\n",
        "  if pad_id is None:\n",
        "    pad_id = 0\n",
        "  a, b = align(a, b)\n",
        "  for i in range(len(a)):\n",
        "    if a[i] == '-' or a[i] is None:\n",
        "      a[i] = pad_id\n",
        "  for i in range(len(b)):\n",
        "    if b[i] == '-'  or b[i] is None:\n",
        "      b[i] = pad_id\n",
        "  if device is None:\n",
        "    return a, b\n",
        "  return torch.tensor(a).view(1, -1).to(device), torch.tensor(b).view(1, -1).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IfSWnum-_Ow"
      },
      "source": [
        "def read_data(path):\n",
        "  result = []\n",
        "  with open(path, 'r') as file:\n",
        "    for line in file:\n",
        "      inp, out = line[3:-5].split(' [end] out: ')\n",
        "      inp = inp.strip()\n",
        "      out = out.strip()\n",
        "      if inp[-1] != '.':\n",
        "        inp += '.'\n",
        "      if out[-1] != '.':\n",
        "        out += '.'\n",
        "      result.append((inp, out))\n",
        "  return result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY9jJP473Kzi"
      },
      "source": [
        "def calc_accuaracy(model, tokenizer, pairs):\n",
        "  simple = 'Я я мы Мы мне Мне мой Мой Моего моего Нам нам'.split(' ')\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    correct = 0 \n",
        "    total = 0\n",
        "    correct_changed = 0 \n",
        "    total_changed = 0\n",
        "\n",
        "    correct_difficult_changed = 0\n",
        "    total_difficult_changed = 0\n",
        "\n",
        "    for pair in pairs:\n",
        "      if True:\n",
        "        input, target = pair\n",
        "        predicted = predict(input)\n",
        "        input2 = input\n",
        "        input, target = get_pair(split_by_words(input), split_by_words(target), '<pad>', device=None)\n",
        "        input, predicted = get_pair(split_by_words(input2), split_by_words(predicted), '<pad>', device=None)\n",
        "\n",
        "        while len(target) > len(input):\n",
        "          target = target[:-1]\n",
        "\n",
        "        while len(input) > len(target):\n",
        "          input = input[:-1]\n",
        "        \n",
        "        while len(predicted) > len(target):\n",
        "          predicted = predicted[:-1]\n",
        "          total += 1\n",
        "\n",
        "        while len(predicted) < len(target):\n",
        "          target = target[:-1]\n",
        "          input = input[:-1]\n",
        "          total += 1\n",
        "\n",
        "        predicted = np.array(predicted)\n",
        "        input = np.array(input)\n",
        "        target = np.array(target)\n",
        "\n",
        "        correct_tensor = predicted == target\n",
        "        is_changed_tensor = input != target\n",
        "\n",
        "        correct += correct_tensor.sum()\n",
        "        total += predicted.shape[0]\n",
        "\n",
        "        correct_changed += (correct_tensor * is_changed_tensor).sum()\n",
        "        total_changed += is_changed_tensor.sum()\n",
        "\n",
        "        for token in simple:\n",
        "          is_changed_tensor *= (input != token)\n",
        "\n",
        "        \n",
        "        correct_difficult_changed += (correct_tensor * is_changed_tensor).sum()\n",
        "        total_difficult_changed += is_changed_tensor.sum()\n",
        "\n",
        "\n",
        "  model.train()\n",
        "  print(total, total_changed, total_difficult_changed)\n",
        "  return correct / total, correct_changed / total_changed, correct_difficult_changed / total_difficult_changed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNYPcAmEQDwU"
      },
      "source": [
        "import torch\n",
        "pairs = read_data('train.txt')\n",
        "acc = calc_accuaracy(model, tokenizer, pairs)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epe4AXboQLIV"
      },
      "source": [
        "# Проверяем полученнную модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-kpMtmoxvQ3a",
        "outputId": "788c78a8-aaac-4e0a-cf90-e5be3e7de287"
      },
      "source": [
        "predict('Ваня мне помог, но я его проигнорировал')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Ваня ему помог, но он Ваню проигнорировал' '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6CspTzPBuOk2",
        "outputId": "5680d272-8ebc-401c-e5a5-dc544fa5f061"
      },
      "source": [
        "predict('Вчера мы думали о том чтобы купить пень но я понял что он слишком дорогой.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Вчера они думали о том чтобы купить пень но она поняла что пень слишком дорогой. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "POBMvDssuOvH",
        "outputId": "d9682683-56af-4a6b-be34-547e0f0219a2"
      },
      "source": [
        "predict('Я захотел поднять вазу. Она была очень хрупкой и стояла очень высоко. Пришел слон и сломал её т. к. у него было плохое настроение.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Он захотел поднять вазу. Ваза была очень хрупкой и стояла очень высоко. Пришел слон и сломал вазу т. к. у слона было плохое настроение. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VH9k7T39uOzq",
        "outputId": "78dbfae4-70af-4aff-fb38-18e45f2e0171"
      },
      "source": [
        "predict('Недавно я у Пети купил машину. Он мне сбросил цену на неё.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Недавно он у Пети купил машину. Петя ему сбросил цену на машину. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Njwr794IuO2O",
        "outputId": "8f3ee257-621e-46f8-a437-37b04bc4b5a1"
      },
      "source": [
        "predict('Я пришел к Ване. Он меня ударил')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Он пришел к Ване. Ваня меня ударил '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "n8wD-beo21hy",
        "outputId": "2d21b11a-8b4b-444f-9b63-c2697e81069d"
      },
      "source": [
        "predict('Я лечу в Индию')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Он летит в Индию '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KFc6Zts425I5",
        "outputId": "3e21dec3-de4c-4bc0-e8fa-b25638b7bb29"
      },
      "source": [
        "predict('Я строитель. Мама хорошая. Она меня любит и много делает для меня')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Он строитель. Мама хорошая. Мама его любит и много делает для него '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}